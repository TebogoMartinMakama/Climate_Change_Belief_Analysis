{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many companies are built around lessening oneâ€™s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis involves natural language processing because it deals with human-written text. You'll have to download a few Python libraries to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# ML Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To train a machine learning model, we need data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1>Load the Data that is required<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data by calling the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Data\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "#train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Tweets\n",
    "\n",
    "This is one of the essential steps in any natural language processing (NLP) task. Data scientists never get filtered, ready-to-use data. To make it workable, there is a lot of processing that needs to happen.\n",
    "\n",
    "## Letter casing: \n",
    "Converting all letters to either upper case or lower case.\n",
    "## Tokenizing: \n",
    "Turning the tweets into tokens. Tokens are words separated by spaces in a text.\n",
    "## Noise removal: \n",
    "Eliminating unwanted characters, such as HTML tags, punctuation marks, special characters, white spaces etc.\n",
    "## Stopword removal: \n",
    "Some words do not contribute much to the machine learning model, so it's good to remove them. A list of stopwords can be defined by the nltk library, or it can be business-specific.\n",
    "## Normalization: \n",
    "Normalization generally refers to a series of related tasks meant to put all text on the same level. Converting text to lower case, removing special characters, and removing stopwords will remove basic inconsistencies. Normalization improves text matching.\n",
    "## Stemming: \n",
    "Eliminating affixes (circumfixes, suffixes, prefixes, infixes) from a word in order to obtain a word stem. Porter Stemmer is the most widely used technique because it is very fast. Generally, stemming chops off end of the word, and mostly it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing URLs... for Train\n",
      "Removing URLs...for Test data\n"
     ]
    }
   ],
   "source": [
    "## Remove urls\n",
    "print ('Removing URLs... for Train')\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "train['message'] = train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "\n",
    "## Remove urls test\n",
    "print ('Removing URLs...for Test data')\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "test['message'] = test['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowering case... Train\n",
      "Lowering case... Test\n"
     ]
    }
   ],
   "source": [
    "# Make lower case\n",
    "print ('Lowering case... Train')\n",
    "train['message'] = train['message'].str.lower()\n",
    "\n",
    "# Make lower case\n",
    "print ('Lowering case... Test')\n",
    "test['message'] = test['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning punctuation... for Test and Train using the below function\n"
     ]
    }
   ],
   "source": [
    "#Noise removal:\n",
    "print ('Cleaning punctuation... for Test and Train using the below function')\n",
    "def remove_punctuation_numbers(post):\n",
    "    punc_numbers = string.punctuation  + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_numbers])\n",
    "train['message'] = train['message'].apply(remove_punctuation_numbers)\n",
    "test['message'] = test['message'].apply(remove_punctuation_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing NonAscii\n"
     ]
    }
   ],
   "source": [
    "#Removed NonAscii\n",
    "print ('removing NonAscii')\n",
    "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "train['message'] = train['message'].apply(_removeNonAscii)\n",
    "test['message'] = test['message'].apply(_removeNonAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing URLs... for Train\n"
     ]
    }
   ],
   "source": [
    "## Remove \\n\n",
    "print ('Removing URLs... for Train')\n",
    "pattern_url = r'\\n'\n",
    "subs_url = r' '\n",
    "train['message'] = train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "test['message'] = test['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Data\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "#train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminating affixes (circumfixes, suffixes, prefixes, infixes) from a word in order to obtain a word stem\n",
    "#ps = PorterStemmer()\n",
    "#stemmed_words = [ps.stem(w) for w in train['message']]\n",
    "#stemmed_words = [ps.stem(w) for w in test['message']]\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in train['message']]\n",
    "#lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in test['message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting you Data for X and Y\n",
    "X = train['message']\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same tf vector will be used for Testing sentiments on unseen trending data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistics Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805751909816737\n",
      "accuracy_score 0.7756005056890013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "# Linear SVC Model:\n",
    "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),('clf', LinearSVC()),])\n",
    "text_clf_lsvc.fit(X_train, y_train)\n",
    "predictions = text_clf_lsvc.predict(X_test)\n",
    "print(f1_score(y_test, predictions,average=\"macro\"))\n",
    "print('accuracy_score',accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the performance Model on the Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805751909816737\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, predictions,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same tf vector will be used for Testing sentiments on unseen trending data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_NT_test = test['message']\n",
    "predicted = text_clf_lsvc.predict(X_NT_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Prediction on unseen test data and adding sentiment column to our original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an ouput csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['tweetid','sentiment']].to_csv('testsubmissionLsvc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
